---
trigger: always_on
---

{
  "messages": [
    {
      "role": "system",
      "content": "====================\nTESTING EXCELLENCE RULES\n====================\n\n**MISSION:** Build confidence through comprehensive testing, catch bugs early, and ensure software quality at every level.\n\n====================\n1) TEST PYRAMID IMPLEMENTATION\n====================\n**Ideal Test Distribution:**\n```\n        /\\           E2E Tests: 10%\n       /  \\          Integration Tests: 20%\n      /    \\         Unit Tests: 70%\n     /______\\\n    /________\\\n```\n\n**Unit Testing Standards:**\n- **Coverage:** >80% overall, >90% critical paths\n- **Isolation:** Test one thing at a time\n- **Naming:** `test_[method]_[scenario]_[expected]`\n- **AAA Pattern:** Arrange, Act, Assert\n- **Fast:** Run entire suite in <5 minutes\n\n**Integration Testing Standards:**\n- Test interactions between components\n- Use real dependencies where possible\n- Mock external services\n- Test error scenarios and edge cases\n- Run in CI pipeline, not locally\n\n**E2E Testing Standards:**\n- Test complete user journeys\n- Use production-like data\n- Run in production-like environment\n- Prioritize critical business flows\n- Avoid flakiness through proper waiting\n\n**Test Pyramid Tools:**\n- **Unit:** Jest (JS), pytest (Python), JUnit (Java)\n- **Integration:** Supertest (API), TestContainers (DB)\n- **E2E:** Cypress, Playwright, Selenium\n- **Mobile:** Appium, Detox, Espresso (Android), XCTest (iOS)\n\n====================\n2) TEST AUTOMATION STRATEGIES\n====================\n**Test Automation Principles:**\n- **Automate Repetitive Tests:** Manual testing for exploratory only\n- **Test Data Management:** Separate test data from test logic\n- **Parallel Execution:** Run tests in parallel for speed\n- **Self-Healing Tests:** Automatically fix minor selectors\n- **Visual Testing:** Screenshot comparison for UI\n\n**Test Automation Framework:**\n```typescript\n// Example test framework structure\nframework/\n├── src/\n│   ├── pages/          # Page objects\n│   ├── components/     # Component objects\n│   ├── fixtures/       # Test data\n│   ├── utils/          # Helper functions\n│   └── types/          # Type definitions\n├── tests/\n│   ├── unit/           # Unit tests\n│   ├── integration/    # Integration tests\n│   └── e2e/            # End-to-end tests\n├── config/             # Environment configs\n└── reports/            # Test reports\n```\n\n**Test Data Strategy:**\n- **Factory Pattern:** Create test data on demand\n- **Fixtures:** Predefined data for common scenarios\n- **Faker Library:** Generate realistic test data\n- **Database Seeding:** Reset to known state before tests\n- **Data Privacy:** Anonymize or generate synthetic data\n\n**Test Environment Management:**\n- **Ephemeral Environments:** Create/destroy for each test run\n- **Docker Compose:** Local environment consistency\n- **Kubernetes Namespaces:** Isolated test environments\n- **Service Virtualization:** Mock external dependencies\n- **Environment Variables:** Configuration for different environments\n\n====================\n3) PERFORMANCE TESTING INTEGRATION\n====================\n**Performance Test Types:**\n\n**Load Testing:**\n- Simulate expected number of concurrent users\n- Measure response times, throughput, error rates\n- Identify performance bottlenecks\n- Tools: k6, JMeter, Gatling\n\n**Stress Testing:**\n- Push system beyond normal limits\n- Find breaking points and failure modes\n- Test recovery procedures\n- Monitor resource utilization (CPU, memory, I/O)\n\n**Soak Testing:**\n- Long duration testing (24-72 hours)\n- Identify memory leaks, resource exhaustion\n- Check stability over time\n- Monitor for gradual performance degradation\n\n**Spike Testing:**\n- Sudden increase in load\n- Test auto-scaling response\n- Measure recovery time\n- Validate monitoring alerts\n\n**Performance Testing in CI/CD:**\n```yaml\n# Example GitHub Actions performance test\nperformance-test:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v2\n    - uses: grafana/k6-action@v0.2.0\n      with:\n        filename: tests/performance/load-test.js\n        flags: --duration 5m --vus 100\n    - name: Upload results\n      uses: actions/upload-artifact@v2\n      with:\n        name: k6-results\n        path: results/\n```\n\n**Performance Metrics Collection:**\n- Response time percentiles (p50, p90, p95, p99)\n- Requests per second (RPS)\n- Error rate under load\n- Resource utilization (CPU, memory, network)\n- Database query performance\n\n====================\n4) SECURITY TESTING INTEGRATION\n====================\n**Security Testing Types:**\n\n**Static Application Security Testing (SAST):**\n- Analyze source code for vulnerabilities\n- Integrate in IDE and CI pipeline\n- Tools: SonarQube, Checkmarx, Fortify\n- Fix critical issues before merge\n\n**Dynamic Application Security Testing (DAST):**\n- Test running application for vulnerabilities\n- Automated scanning of endpoints\n- Tools: OWASP ZAP, Burp Suite\n- Schedule regular scans\n\n**Software Composition Analysis (SCA):**\n- Scan dependencies for known vulnerabilities\n- Automatically update dependencies\n- Tools: Snyk, Dependabot, WhiteSource\n- Block merges with critical vulnerabilities\n\n**Infrastructure as Code Scanning:**\n- Scan Terraform, CloudFormation, Kubernetes manifests\n- Check for security misconfigurations\n- Tools: Checkov, Terrascan, KICS\n- Enforce security policies\n\n**Container Security Scanning:**\n- Scan Docker images for vulnerabilities\n- Check for best practices\n- Tools: Trivy, Clair, Docker Scout\n- Block deployment of vulnerable images\n\n**Security Testing Pipeline:**\n```yaml\nsecurity-pipeline:\n  stages:\n    - sast:\n        tools: [sonarqube, semgrep]\n    - sca:\n        tools: [snyk, dependabot]\n    - container-scan:\n        tools: [trivy, grype]\n    - iac-scan:\n        tools: [checkov, tfsec]\n    - dast:\n        tools: [zap, nuclei]\n  gates:\n    - critical_vulnerabilities: 0\n    - high_vulnerabilities: <5\n    - medium_vulnerabilities: <20\n```\n\n====================\n5) CHAOS ENGINEERING\n====================\n**Chaos Engineering Principles:**\n- **Build Hypothesis:** What should happen during failure\n- **Vary Real-world Events:** Simulate production scenarios\n- **Run in Production:** Test in real environment\n- **Automate Experiments:** Run regularly\n- **Minimize Blast Radius:** Start small, limit impact\n\n**Chaos Experiments:**\n\n**Network Chaos:**\n- Latency injection (100ms, 500ms, 1000ms)\n- Packet loss (1%, 5%, 10%)\n- Network partition\n- DNS failure\n\n**Infrastructure Chaos:**\n- Terminate instances/containers\n- CPU/Memory stress\n- Disk space exhaustion\n- Clock skew\n\n**Application Chaos:**\n- Throw exceptions at random\n- Slow down responses\n- Return error responses\n- Crash processes\n\n**Dependency Chaos:**\n- Slow downstream services\n- Return errors from dependencies\n- Timeout external calls\n- Simulate third-party outages\n\n**Chaos Engineering Tools:**\n- **Chaos Mesh:** Kubernetes-native chaos engineering\n- **Litmus:** Cloud-native chaos engineering\n- **Gremlin:** Enterprise chaos engineering\n- **AWS Fault Injection Simulator:** AWS-specific chaos\n- **Chaos Toolkit:** Multi-platform chaos experiments\n\n**Chaos Experiment Template:**\n```yaml\nexperiment:\n  title: \"Database connection failure\"\n  description: \"Simulate database connection timeout\"\n  hypothesis: \"Application should show graceful error message\"\n  method:\n    - type: network\n      action: delay\n      target: database-service\n      delay: \"5s\"\n    - type: network\n      action: loss\n      target: database-service\n      loss: \"100%\"\n  duration: \"2m\"\n  rollback: true\n  metrics:\n    - error_rate\n    - response_time\n    - user_experience_score\n```\n\n====================\n6) TEST DATA MANAGEMENT\n====================\n**Test Data Principles:**\n- **Isolation:** Tests shouldn't affect each other\n- **Repeatability:** Same results every run\n- **Performance:** Fast test data setup\n- **Privacy:** No real user data\n- **Maintainability:** Easy to update test data\n\n**Test Data Generation:**\n- **Factory Pattern:** Programmatically create data\n- **Fixtures:** JSON/CSV files with test data\n- **Faker Libraries:** Generate realistic fake data\n- **Database Dumps:** Sanitized production snapshots\n- **Golden Masters:** Known-good output for comparison\n\n**Test Data Privacy:**\n- **Anonymization:** Remove/modify PII\n- **Synthetic Data:** Algorithmically generated\n- **Data Masking:** Hide sensitive fields\n- **Consent Management:** Ensure compliance\n- **Audit Trails:** Track test data usage\n\n**Test Data for Different Environments:**\n- **Local:** Small dataset, fast setup\n- **CI:** Medium dataset, balanced\n- **Staging:** Large dataset, production-like\n- **Performance:** Very large dataset, stress testing\n- **Security:** Specific data for security tests\n\n**Test Data Management Tools:**\n- **Database Tools:** Flyway, Liquibase for migrations\n- **Data Generation:** Faker, Mockaroo, DataFactory\n- **Data Masking:** Delphix, Informatica\n- **Containerization:** Testcontainers for isolated DBs\n- **Orchestration:** Custom scripts, Makefiles\n\n====================\n7) ACCESSIBILITY TESTING\n====================\n**Accessibility Standards:**\n- **WCAG 2.1:** Level AA compliance required\n- **Section 508:** US federal requirements\n- **ADA:** Americans with Disabilities Act\n- **EN 301 549:** European standard\n\n**Automated Accessibility Testing:**\n- **Tools:** axe-core, Lighthouse, Pa11y\n- **Integration:** In CI pipeline and local development\n- **Coverage:** All pages and critical user flows\n- **Reporting:** Clear, actionable reports\n\n**Manual Accessibility Testing:**\n- **Keyboard Navigation:** Tab through all interactive elements\n- **Screen Reader Testing:** NVDA, JAWS, VoiceOver\n- **Color Contrast:** Verify sufficient contrast ratios\n- **Zoom Testing:** 200% zoom functionality\n- **Focus Management:** Logical focus order\n\n**Accessibility Testing Checklist:**\n```markdown\n## WCAG 2.1 AA Checklist\n\n### Perceivable\n- [ ] Text alternatives for non-text content\n- [ ] Captions and other alternatives for multimedia\n- [ ] Content can be presented in different ways\n- [ ] Content is easier to see and hear\n\n### Operable\n- [ ] All functionality available from keyboard\n- [ ] Users have enough time to read and use content\n- [ ] Content does not cause seizures\n- [ ] Users can navigate and find content\n\n### Understandable\n- [ ] Text is readable and understandable\n- [ ] Content appears and operates predictably\n- [ ] Users are helped to avoid and correct mistakes\n\n### Robust\n- [ ] Content is compatible with current and future tools\n```\n\n**Accessibility Testing Tools:**\n- **Automated:** axe DevTools, WAVE, Accessibility Insights\n- **Screen Readers:** NVDA (Windows), VoiceOver (macOS/iOS)\n- **Color Contrast:** Color Contrast Analyzer, Stark\n- **Browser Extensions:** axe, Wave, Accessibility Checker\n- **CI/CD Integration:** Pa11y CI, axe-core integrations\n\n====================\n8) TEST REPORTING & METRICS\n====================\n**Test Reporting Requirements:**\n- **Real-time:** Show test execution progress\n- **Historical:** Track trends over time\n- **Actionable:** Highlight failures and flaky tests\n- **Accessible:** Available to all stakeholders\n- **Integratable:** With other tools (Jira, Slack)\n\n**Key Test Metrics:**\n- **Test Coverage:** Code coverage percentages\n- **Test Execution Time:** Duration of test runs\n- **Pass/Fail Rate:** Percentage of tests passing\n- **Flaky Tests:** Number of non-deterministic tests\n- **Defect Escape Rate:** Bugs found in production\n- **Test Maintenance Cost:** Time spent maintaining tests\n\n**Test Dashboard Components:**\n```yaml\ndashboard:\n  overview:\n    - total_tests: 1500\n    - passing: 98%\n    - coverage: 85%\n    - execution_time: 12m\n  trends:\n    - 